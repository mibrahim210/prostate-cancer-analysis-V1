{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 (default, Oct  8 2019, 13:06:37) \n",
      "[GCC 5.4.0 20160609]\n",
      "NumPy 1.16.4\n",
      "SciPy 1.3.0\n",
      "Scikit-Learn 0.20.3\n",
      "Matplotlib 2.0.0\n"
     ]
    }
   ],
   "source": [
    "#loading libraries\n",
    "import platform; \n",
    "import sys;\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from skimage import measure, morphology\n",
    "%matplotlib inline\n",
    "#loading libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sklearn as sl\n",
    "import matplotlib\n",
    "import os\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from skimage import measure, morphology\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.metrics import geometric_mean_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "#fix random seed for reproducibility\n",
    "RANDOM_STATE_VALUE = 1234\n",
    "\n",
    "print(\"Python\", sys.version)\n",
    "print(\"NumPy\", np.__version__)\n",
    "print(\"SciPy\", scipy.__version__)\n",
    "print(\"Scikit-Learn\", sl.__version__)\n",
    "print(\"Matplotlib\",matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util Functions\n",
    "\n",
    "#plot values generated by SMOTE\n",
    "def plot_2d_space(X, y, label='Classes'):   \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "        \n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "def plot_2d_space_multiple(X,y,label,ax):   \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        ax.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    ax.set_title(label)    \n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "def do_sampling_tomelinks(train_X,target_Y,ratio):\n",
    "    tl = TomekLinks(return_indices=True, ratio=ratio)\n",
    "    X_tl, y_tl, id_tl = tl.fit_sample(train_X, target_Y)\n",
    "    return X_tl,y_tl,id_tl\n",
    "\n",
    "def convert_to_encoding(target_y):\n",
    "    data_encoder = LabelEncoder()\n",
    "    data_encoder.fit(target_y)\n",
    "    encoded_y = data_encoder.transform(target_y)\n",
    "    return encoded_y\n",
    "\n",
    "def do_sampling_smote(train_X,target_Y,sampling_strategy = 'not minority',smoteenn=True):\n",
    "    if smoteenn:\n",
    "        smot_sampler = SMOTEENN(random_state=RANDOM_STATE_VALUE)\n",
    "        X_res, Y_res = smot_sampler.fit_resample(train_X, convert_to_encoding(target_Y))\n",
    "    else:\n",
    "        #no_value=True\n",
    "        smot_sampler= SMOTE(random_state=RANDOM_STATE_VALUE,ratio='minority')\n",
    "        #smot_sampler= BorderlineSMOTE(sampling_strategy=sampling_strategy,random_state=RANDOM_STATE_VALUE)\n",
    "        #smot_sampler = RandomOverSampler(random_state=RANDOM_STATE_VALUE)\n",
    "        X_res, Y_res = smot_sampler.fit_resample(train_X, convert_to_encoding(target_Y))\n",
    "    return X_res,Y_res\n",
    "        \n",
    "\n",
    "    \n",
    "def set_ggplot_style():\n",
    "    plt.style.use('ggplot')\n",
    "    \n",
    "def list_of_styles():\n",
    "    print(plt.style.available)\n",
    "\n",
    "    \n",
    "def view_tabular_feature_importances():\n",
    "    pass\n",
    "\n",
    "def set_plot_size(width,height):\n",
    "    plt.rcParams['figure.figsize'] = [width,height]\n",
    "    \n",
    "def do_lgb_on_sampled_data(train_X,train_Y,regression=True):\n",
    "    if regression:\n",
    "        gbm = lgb.LGBMRegressor()\n",
    "    else:\n",
    "        #gbm = lgb.LGBMClassifier(class_weight='balanced')\n",
    "        #gbm = lgb.LGBMClassifier(scale_pos_weight=1.5)\n",
    "        gbm = lgb.LGBMClassifier()\n",
    "    return gbm.fit(train_X, train_Y)\n",
    "\n",
    "def do_xgb_on_sampled_data(train_X,train_Y,regression=True):\n",
    "    if regression :\n",
    "        print(\"TO DO\")\n",
    "    else:\n",
    "        xgb_tuned = xgb.XGBClassifier(max_depth=20, n_estimators=200,min_child_weight= 1, learning_rate=0.2,subsample=0.6,\n",
    "colsample_bytree=0.8,gamma=1,eval_metric='logloss',scale_pos_weight=1.8)\n",
    "    return xgb_tuned.fit(train_X, train_Y)\n",
    "\n",
    "def do_random_forest():\n",
    "    rf_grid_searched_dose = RandomForestClassifier(bootstrap= True,criterion='entropy',max_depth=20,max_features='auto',min_samples_leaf=5,min_samples_split=8,n_estimators=300,random_state=RANDOM_STATE_VALUE)\n",
    "    rf_vanila_dose = RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE_VALUE, n_jobs=-1)\n",
    "    return rf_vanila_dose\n",
    "\n",
    "def do_catboost_on_sampled_data(train_X,train_Y,regression=True):\n",
    "    if regression:\n",
    "        print(\"TO DO \")\n",
    "    else :\n",
    "        #model_cat = CatBoostClassifier(eval_metric='BalancedAccuracy',use_best_model=True,random_seed=1234)\n",
    "        model_cat = CatBoostClassifier(eval_metric='BalancedAccuracy',random_seed=1234)\n",
    "    return model_cat.fit(train_X, train_Y)   \n",
    "        \n",
    "def create_confusion_matrix(predictions,target_y):\n",
    "    encoded_y = convert_to_encoding(target_y) \n",
    "    cm = confusion_matrix(encoded_y, predictions)\n",
    "    return cm,encoded_y\n",
    "    \n",
    "def predict_and_create_cm(model,target_x,target_y):\n",
    "    encoded_y = convert_to_encoding(target_y) \n",
    "    predictions = model.predict(target_x)\n",
    "    predictions=predictions.argmax(axis=-1)\n",
    "    cm = confusion_matrix(encoded_y, predictions)\n",
    "    return cm,predictions,encoded_y\n",
    "\n",
    "def predict_proba_model(model,target_X):\n",
    "    predictions_probability = model.predict_proba(target_X)\n",
    "    return predictions_probability\n",
    "\n",
    "#calculcate sensitivity and specificity\n",
    "def calculate_custom_sensitivity(cm):\n",
    "    tp = cm[1,1]\n",
    "    tn = cm[0,0]\n",
    "    fp= cm[0,1]\n",
    "    fn=cm[1,0]\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = tp/(tp+fn)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = tn/(tn+fp)\n",
    "    return TPR,TNR\n",
    "\n",
    "#calculate \n",
    "def plot_confusion_matrix_with_report(cm, classes,ax,bc,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    print('')\n",
    "    \n",
    "    #calculate sensitivity and specificity\n",
    "    sensitivity,specificity= calculate_custom_sensitivity(cm)\n",
    " \n",
    "    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.sca(ax)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, format(cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label\\n bac={:0.2f};sen={:0.2f}:spe={:0.2f}'.format(bc,round(sensitivity,2),round(specificity,2))) \n",
    "    ax.grid(False)\n",
    "\n",
    "\n",
    "\n",
    "def apply_heuristics_on_generate_data(generated_data_x,generated_data_y,columns_list,train_dummy_X,test_dummy_X,target_train,target_test):\n",
    "    data_frame_generated = pd.DataFrame(generated_data_x,columns=columns_list)\n",
    "    data_frame_generated['36months'] = generated_data_y\n",
    "    \n",
    "    train_dummy_X['36months'] = target_train\n",
    "    test_dummy_X['36months'] = target_test\n",
    "    \n",
    "    merged_dataframe_test = pd.concat([data_frame_generated, test_dummy_X]) \n",
    "    merged_dataframe_test.drop_duplicates(keep=False, inplace=True)\n",
    "  \n",
    "    \n",
    "    merged_dataframe_train = pd.concat([train_dummy_X, merged_dataframe_test]) \n",
    "    \n",
    "    return data_frame_generated,merged_dataframe_test,merged_dataframe_train\n",
    " \n",
    "def get_single_count_from_data_frame (df,variable_name):\n",
    "    no_,yes_ = df[variable_name].value_counts()\n",
    "    return no_,yes_\n",
    "\n",
    "def apply_pseudo_labeling(model,target_X,proba_score,train_X,train_Y):\n",
    "    predictions_in_probability = predict_proba_model(model,target_X)\n",
    "    predictions_in_probability_modified = (predictions_in_probability [:,1] >= proba_score).astype('int')\n",
    "    prediction_dataframe = pd.DataFrame(predictions_in_probability_modified,columns=['36months'])\n",
    "    augmented_test = target_X.copy(deep=True)\n",
    "    augmented_test['36months'] = predictions_in_probability_modified\n",
    "    \n",
    "    \n",
    "    augmented_test_label= augmented_test['36months']\n",
    "    augmented_set_labels = augmented_test.pop('36months')\n",
    "    new_train_X = np.vstack((train_X, augmented_test.values))\n",
    "    new_train_Y = np.concatenate((train_Y, augmented_test_label.values), axis=0)\n",
    "    \n",
    "    return prediction_dataframe,augmented_test,new_train_X,new_train_Y\n",
    "\n",
    "def create_merged_data_frame(model,target_X,generated_data_x,generated_data_y,columns_list):\n",
    "    predictions_in_probability = model.predict(target_X)\n",
    "    predictions_in_probability=predictions_in_probability.argmax(axis=-1)\n",
    "        \n",
    "    predictions_in_probability_modified = predictions_in_probability.astype('int')\n",
    "    \n",
    "    prediction_dataframe = pd.DataFrame(predictions_in_probability_modified,columns=['36months'])\n",
    "    augmented_test = target_X.copy(deep=True)\n",
    "    augmented_test['36months'] = predictions_in_probability_modified\n",
    "    \n",
    "    data_frame_generated = pd.DataFrame(generated_data_x,columns=columns_list)\n",
    "    data_frame_generated['36months'] = generated_data_y\n",
    "    \n",
    "    merged_dataframe_train = pd.concat([data_frame_generated, augmented_test])\n",
    "    \n",
    "    return merged_dataframe_train    \n",
    "    \n",
    "    \n",
    "\n",
    "def pop_labels(dataframe,label_name):\n",
    "    dataframe_label = dataframe[label_name]\n",
    "    dataframe.pop(label_name)\n",
    "    return dataframe,dataframe_label\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "\n",
    "def merge_dataframe_only(list_of_dfs):\n",
    "    merged =pd.concat([merged_df_rf,merged_df_xgb,merged_df_lgb,merged_df_cat]) \n",
    "    merged.drop_duplicates(keep='first', inplace=True)\n",
    "    merged = clean_dataset(merged)\n",
    "    return shuffle(merged)\n",
    "\n",
    "def find_best_cut_off(true_Y,predicted_y):\n",
    "    fpr, tpr, thresholds = roc_curve(true_Y,predicted_y[:,1],pos_label=1)\n",
    "    optimal_idx=np.argmin(np.sqrt(np.square(1-tpr) + np.square(fpr)))\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold\n",
    "\n",
    "def draw_roc_curve(Y,probs,title):\n",
    "    # Perforamnce of the model\n",
    "    fpr, tpr, thresholds = roc_curve(Y,probs,pos_label=1)\n",
    "    #print('tpr: ', tpr)\n",
    "    roc_score = roc_auc_score (Y,probs)\n",
    "    AUC  = auc(fpr, tpr)\n",
    "    print ('The AUC is : %0.4f' %  AUC)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.4f)' % AUC)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for %s' %title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def prepare_data_set_cox_analysis_and_classification(data_main):\n",
    "    train_data_temp = data_main[data_main.cohort== 1]\n",
    "    test_data_temp= data_main[data_main.cohort== 2]\n",
    "    \n",
    "    print('Procesing Train Data for cox analysis and classification :' ,len(train_data_temp))\n",
    "    print('Procesing Test Data: for cox analysis and classification :' ,len(test_data_temp))\n",
    "\n",
    "    train_data_temp = shuffle(train_data_temp)\n",
    "    test_data_temp = shuffle(test_data_temp)\n",
    "\n",
    "    test_data_temp['36months']= test_data_temp['36months'].astype('bool')\n",
    "\n",
    "    train_X_temp = train_data_temp.iloc[:,0:90].astype(float)\n",
    "    train_temp_y = train_data_temp.iloc[:,2].astype(float)\n",
    "    \n",
    "    del train_X_temp['36months']\n",
    "    del train_X_temp['cohort']\n",
    "    del train_X_temp['bleeded']\n",
    "\n",
    "    test_X_temp = test_data_temp.iloc[:,0:90].astype(float)\n",
    "    test_temp_y = test_data_temp.iloc[:,[0,2]]\n",
    "    \n",
    "    del test_X_temp['36months']\n",
    "    del test_X_temp['months']\n",
    "    del test_X_temp['cohort']\n",
    "    del test_X_temp['bleeded']\n",
    "    \n",
    "    return train_X_temp,train_temp_y,test_X_temp,test_temp_y\n",
    "\n",
    "\n",
    "def create_data_set_for_cox_and_classification_after_smote (X_res_train_temp,Y_res_train_temp,columns_list):\n",
    "    data_frame_generated = pd.DataFrame(X_res_train_temp,columns=columns_list)\n",
    "    data_frame_generated['36months'] = Y_res_train_temp\n",
    "    data_frame_cox_target = data_frame_generated.iloc[:,[0,87]]\n",
    "    #del data_frame_generated['months']\n",
    "    return data_frame_generated,data_frame_cox_target\n",
    "\n",
    "def plot_feature_importances(model,feature_names,number_of_features,title):\n",
    "    plt.rcParams['figure.figsize'] = [10,8]\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=feature_names.columns)\n",
    "    feat_importances.nlargest(20).plot(kind='barh',color=colors)\n",
    "    plt.title('First {} Important Features for {}'.format(number_of_features, title) , fontsize = 20)\n",
    "    plt.yticks(fontsize = 10)\n",
    "    plt.ylabel('Name of the features',fontsize = 20)\n",
    "    \n",
    "    \n",
    "#helper function to generate color palette\n",
    "import random\n",
    "\n",
    "def get_random_color(pastel_factor = 0.7):\n",
    "    return [(x+pastel_factor)/(1.0+pastel_factor) for x in [random.uniform(0,1.0) for i in [1,2,3]]]\n",
    "\n",
    "def color_distance(c1,c2):\n",
    "    return sum([abs(x[0]-x[1]) for x in zip(c1,c2)])\n",
    "\n",
    "def generate_new_color(existing_colors,pastel_factor = 0.5):\n",
    "    max_distance = None\n",
    "    best_color = None\n",
    "    for i in range(0,100):\n",
    "        color = get_random_color(pastel_factor = pastel_factor)\n",
    "        if not existing_colors:\n",
    "            return color\n",
    "        best_distance = min([color_distance(color,c) for c in existing_colors])\n",
    "        if not max_distance or best_distance > max_distance:\n",
    "            max_distance = best_distance\n",
    "            best_color = color\n",
    "    return best_color\n",
    "\n",
    "colors = []\n",
    "for i in range(0,30):\n",
    "    colors.append(generate_new_color(colors,pastel_factor = 0.8))\n",
    "    \n",
    "def get_important_features_list (model,columns_list,num_of_features):\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=columns_list)\n",
    "    df=feat_importances.nlargest(num_of_features)\n",
    "    important_features_list = []\n",
    "    \n",
    "    for i, v in df.items():\n",
    "        important_features_list.append(i)\n",
    "    return important_features_list\n",
    "\n",
    "def do_tsne(target_X,number_of_components):\n",
    "    tsne = TSNE(n_components=number_of_components,random_state=RANDOM_STATE_VALUE)\n",
    "    #transformed_2d = tsne.fit_transform(target_X)\n",
    "    return tsne.fit_transform(target_X)\n",
    "\n",
    "def apply_rf(params,default=False):\n",
    "    if default:\n",
    "        rf_clf= RandomForestClassifier(random_state=RANDOM_STATE_VALUE)\n",
    "    else:\n",
    "        rf_clf= RandomForestClassifier(**params)\n",
    "    return rf_clf\n",
    "\n",
    "\n",
    "def apply_grid_search_rf(target_X,target_Y):\n",
    "    rfc=RandomForestClassifier(random_state=RANDOM_STATE_VALUE,n_jobs=-1)\n",
    "    param_grid = { \n",
    "    'bootstrap': [True],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [20,30,40,50,60,70,80,90,100],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100,150,200,250,300,400,500],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "    scorers = {\n",
    "    #'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'balanced_accuracy_score': make_scorer(balanced_accuracy_score)\n",
    "    }\n",
    "\n",
    "    refit_score = 'recall_score'\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True,random_state=RANDOM_STATE_VALUE)    \n",
    "    rf_grid = GridSearchCV(rfc, param_grid, cv=kfold,scoring=scorers,verbose=1,refit=refit_score,n_jobs=-1)\n",
    "\n",
    "    rf_grid.fit(target_X, target_Y)\n",
    "    \n",
    "    return rf_grid\n",
    "    \n",
    "\n",
    "def draw_roc_curve_multiple(Y,probs,title):\n",
    "    # Perforamnce of the model\n",
    "    fpr, tpr, thresholds = roc_curve(Y,probs,pos_label=1)\n",
    "    #print('tpr: ', tpr)\n",
    "    roc_score = roc_auc_score (Y,probs)\n",
    "    AUC  = auc(fpr, tpr)\n",
    "    print ('The AUC is : %0.4f' %  AUC)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.4f)' % AUC)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for %s' %title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "def do_pca(target_X,number_of_components):\n",
    "    pca = PCA(n_components=number_of_components)\n",
    "    return pca.fit_transform(target_X)\n",
    "\n",
    "def create_data_frame_from_sampling_array(generated_data_x,generated_data_y):\n",
    "    data_frame_generated = pd.DataFrame(generated_data_x)\n",
    "    data_frame_generated['36months'] = generated_data_y\n",
    "    \n",
    "    target_Y = data_frame_generated['36months']\n",
    "    data_frame_generated.pop('36months')\n",
    "\n",
    "    return data_frame_generated,target_Y\n",
    "\n",
    "\n",
    "def create_data_frame_from_sampling_array_for_covariate(generated_data_x,generated_data_y,label):\n",
    "    data_frame_generated = pd.DataFrame(generated_data_x)\n",
    "    data_frame_generated[label] = generated_data_y\n",
    "    return data_frame_generated\n",
    "\n",
    "def get_important_features_list (model,columns_list,num_of_features):\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=columns_list)\n",
    "    df=feat_importances.nlargest(num_of_features)\n",
    "    important_features_list = []\n",
    "    \n",
    "    for i, v in df.items():\n",
    "        important_features_list.append(i)\n",
    "    return important_features_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:  337\n",
      "Test Data:  254\n"
     ]
    }
   ],
   "source": [
    "#data pre-processing\n",
    "INPUT_FOLDER = 'rennes_prostate_cancer/input_data/'\n",
    "OUTPUT_FOLDER = 'rennes_prostate_cancer/output_results/'\n",
    "data_main = pd.read_excel(INPUT_FOLDER+'data.xls', index_col=None)\n",
    "\n",
    "conditions = [\n",
    "    (data_main['months'] < 36) & (data_main['bleeded']== 1.00)]\n",
    "choices = [1.00]\n",
    "\n",
    "data_main['36months'] = np.select(conditions, choices, default=0.00)\n",
    "\n",
    "#data_main.to_csv(OUTPUT_FOLDER+'prepared_dataset.csv', encoding='utf-8', index=False) un comment if you want something else\n",
    "\n",
    "train_data = data_main[data_main.cohort== 1]\n",
    "test_data= data_main[data_main.cohort== 2]\n",
    "print('Train Data: ' ,len(train_data))\n",
    "print('Test Data: ' ,len(test_data))\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "train_data = shuffle(train_data)\n",
    "test_data = shuffle(test_data)\n",
    "\n",
    "#preparing training set\n",
    "train_X = train_data.iloc[:,0:90].astype(float)\n",
    "#train_temp_y = train_X.iloc[:,2].astype(float)\n",
    "#target_train_X = train_data['36months']\n",
    "#print('Labels of Trained data: ',len(train_temp_y))\n",
    "#del train_X['36months']\n",
    "del train_X['cohort']\n",
    "del train_X['months']\n",
    "del train_X['bleeded']\n",
    "train_X.head()\n",
    "#train_X.to_csv(OUTPUT_FOLDER+'prepared_dataset_test.csv', encoding='utf-8', index=False) #un comment if you want something else\n",
    "\n",
    "#preparing testing set\n",
    "test_X = test_data.iloc[:,0:90].astype(float)\n",
    "test_temp_y = test_X.iloc[:,2].astype(float)\n",
    "#target_test_X = test_data['36months']\n",
    "#print('Labels of Test data: ' , len(test_temp_y))\n",
    "#del test_X['36months']\n",
    "del test_X['cohort']\n",
    "del test_X['months']\n",
    "del test_X['bleeded']\n",
    "test_X.head()\n",
    "\n",
    "target_train = train_X['36months']\n",
    "target_test = test_X['36months']\n",
    "\n",
    "train_set_labels = train_X.pop('36months')\n",
    "test_set_labels = test_X.pop('36months')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC for train and test distributions: 97.490% (2.838%)\n",
      "\n",
      "Resampled dataset for training shape Counter({0: 337, 1: 337})\n",
      "Resampled dataset for testing shape Counter({0: 297, 1: 283})\n",
      "(674, 87)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>antticoagulant_i</th>\n",
       "      <th>hormono</th>\n",
       "      <th>dose_total in the prostate</th>\n",
       "      <th>imrt_only (0=no, 1=yes)</th>\n",
       "      <th>imr_plus_igrt</th>\n",
       "      <th>V0</th>\n",
       "      <th>V1= Volume of the rectum in % receiving a minimum of 1 Gy</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>...</th>\n",
       "      <th>V70</th>\n",
       "      <th>V71</th>\n",
       "      <th>V72</th>\n",
       "      <th>V73</th>\n",
       "      <th>V74</th>\n",
       "      <th>V75</th>\n",
       "      <th>V76</th>\n",
       "      <th>V77</th>\n",
       "      <th>V78</th>\n",
       "      <th>V79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.819178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.6853</td>\n",
       "      <td>...</td>\n",
       "      <td>14.9523</td>\n",
       "      <td>13.5878</td>\n",
       "      <td>12.1970</td>\n",
       "      <td>10.66100</td>\n",
       "      <td>7.23201</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.025650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.509589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.2546</td>\n",
       "      <td>11.8597</td>\n",
       "      <td>10.2528</td>\n",
       "      <td>8.54631</td>\n",
       "      <td>5.85300</td>\n",
       "      <td>1.245910</td>\n",
       "      <td>0.022334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  antticoagulant_i  hormono  dose_total in the prostate  \\\n",
       "0  65.819178               0.0      1.0                        78.0   \n",
       "1  61.509589               0.0      1.0                        80.0   \n",
       "\n",
       "   imrt_only (0=no, 1=yes)  imr_plus_igrt     V0  \\\n",
       "0                      1.0            0.0  100.0   \n",
       "1                      1.0            0.0  100.0   \n",
       "\n",
       "   V1= Volume of the rectum in % receiving a minimum of 1 Gy     V2        V3  \\\n",
       "0                                              100.0          100.0   99.6853   \n",
       "1                                              100.0          100.0  100.0000   \n",
       "\n",
       "  ...       V70      V71      V72       V73      V74       V75       V76  V77  \\\n",
       "0 ...   14.9523  13.5878  12.1970  10.66100  7.23201  0.989538  0.025650  0.0   \n",
       "1 ...   13.2546  11.8597  10.2528   8.54631  5.85300  1.245910  0.022334  0.0   \n",
       "\n",
       "   V78  V79  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "\n",
       "[2 rows x 86 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe1101254a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prorepare data for covariate shift and visualize it \n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "covariate_train_X = train_X.copy()\n",
    "covariate_test_X = test_X.copy()\n",
    "\n",
    "covariate_train_X['is_train'] = 1\n",
    "covariate_test_X['is_train'] = 0 \n",
    "\n",
    "\n",
    "covariate_combine = pd.concat([covariate_train_X, covariate_test_X], axis=0, ignore_index=True)\n",
    "covariate_combine_target=covariate_combine['is_train']\n",
    "\n",
    "\n",
    "Y = covariate_combine['is_train'].values #labels\n",
    "X = covariate_combine.drop('is_train', axis=1).values #covariates or our independent variables\n",
    "\n",
    "\n",
    "covariate_combine_lbl = covariate_combine.pop('is_train')\n",
    "covariate_combine.head(2)\n",
    "\n",
    "clf_co = RandomForestClassifier(n_jobs=-1,random_state=RANDOM_STATE_VALUE)\n",
    "\n",
    "refit_score = 'roc_auc'\n",
    "skfold = SKF(n_splits=20, shuffle=True, random_state=RANDOM_STATE_VALUE)\n",
    "roc_score_cs = cross_val_score(clf_co, X, Y, scoring=refit_score, cv = skfold)\n",
    "\n",
    "print(\"ROC-AUC for train and test distributions: %.3f%% (%.3f%%)\" % (roc_score_cs.mean()*100.0, roc_score_cs.std()*100.0))\n",
    "\n",
    "#print confusion matrix for the distribution\n",
    "clf_co.fit(X,Y)\n",
    "predictions = clf_co.predict(X)\n",
    "bac = balanced_accuracy_score(Y, predictions)\n",
    "cm = confusion_matrix(Y, predictions.round())\n",
    "\n",
    "#visualize confustion matrix\n",
    "fig, ax = plt.subplots(ncols=1,figsize=(8,8))\n",
    "plot_confusion_matrix_with_report(cm, classes=['Testing Set', 'Training Set'],ax=ax,bc=bac.round(),\n",
    "                      title='Covariate shift in the COHORT')\n",
    "\n",
    "\n",
    "\n",
    "#applying SMOTE and SMOTEENN\n",
    "X_res_cov_smote,Y_res_cov_smote = do_sampling_smote(covariate_combine,covariate_combine_target,sampling_strategy = 'not minority',smoteenn=False)\n",
    "print('Resampled dataset for training shape %s' % Counter(Y_res_cov_smote))\n",
    "\n",
    "X_res_cov_smoteenn,Y_res_cov_smoteenn = do_sampling_smote(covariate_combine,covariate_combine_target,sampling_strategy = 'not minority',smoteenn=True)\n",
    "print('Resampled dataset for testing shape %s' % Counter(Y_res_cov_smoteenn))\n",
    "\n",
    "\n",
    "cov_df_smote = pd.DataFrame(X_res_cov_smote,columns=covariate_combine.columns)\n",
    "cov_df_smote['is_train'] = Y_res_cov_smote\n",
    "print(cov_df_smote.shape)\n",
    "\n",
    "cov_df_smote_train_data = cov_df_smote[cov_df_smote.is_train== 1]\n",
    "cov_df_smote_test_data = cov_df_smote[cov_df_smote.is_train== 0]\n",
    "\n",
    "smote_lbl_cov_tr = cov_df_smote_train_data.pop('is_train')\n",
    "smote_lbl_cov_ts = cov_df_smote_test_data.pop('is_train')\n",
    "\n",
    "cov_df_smote_train_data.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesing Train Data for cox analysis and classification : 337\n",
      "Procesing Test Data: for cox analysis and classification : 254\n",
      "Resampled dataset shape for training set:  Counter({1: 308, 0: 299})\n",
      "Preparing Training set after applying SMOTEENN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>antticoagulant_i</th>\n",
       "      <th>hormono</th>\n",
       "      <th>dose_total in the prostate</th>\n",
       "      <th>imrt_only (0=no, 1=yes)</th>\n",
       "      <th>imr_plus_igrt</th>\n",
       "      <th>V0</th>\n",
       "      <th>V1= Volume of the rectum in % receiving a minimum of 1 Gy</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>...</th>\n",
       "      <th>V70</th>\n",
       "      <th>V71</th>\n",
       "      <th>V72</th>\n",
       "      <th>V73</th>\n",
       "      <th>V74</th>\n",
       "      <th>V75</th>\n",
       "      <th>V76</th>\n",
       "      <th>V77</th>\n",
       "      <th>V78</th>\n",
       "      <th>V79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.293151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.6192</td>\n",
       "      <td>95.0677</td>\n",
       "      <td>...</td>\n",
       "      <td>13.3687</td>\n",
       "      <td>12.0047</td>\n",
       "      <td>10.5074</td>\n",
       "      <td>8.66351</td>\n",
       "      <td>5.94861</td>\n",
       "      <td>2.14517</td>\n",
       "      <td>0.346690</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74.767123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.9554</td>\n",
       "      <td>98.5912</td>\n",
       "      <td>...</td>\n",
       "      <td>14.2217</td>\n",
       "      <td>12.8279</td>\n",
       "      <td>11.1897</td>\n",
       "      <td>9.18956</td>\n",
       "      <td>6.65126</td>\n",
       "      <td>2.68426</td>\n",
       "      <td>0.125949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  antticoagulant_i  hormono  dose_total in the prostate  \\\n",
       "0  71.293151               0.0      1.0                        80.0   \n",
       "1  74.767123               0.0      0.0                        80.0   \n",
       "\n",
       "   imrt_only (0=no, 1=yes)  imr_plus_igrt     V0  \\\n",
       "0                      1.0            0.0  100.0   \n",
       "1                      1.0            0.0  100.0   \n",
       "\n",
       "   V1= Volume of the rectum in % receiving a minimum of 1 Gy       V2  \\\n",
       "0                                              100.0          98.6192   \n",
       "1                                              100.0          99.9554   \n",
       "\n",
       "        V3 ...       V70      V71      V72      V73      V74      V75  \\\n",
       "0  95.0677 ...   13.3687  12.0047  10.5074  8.66351  5.94861  2.14517   \n",
       "1  98.5912 ...   14.2217  12.8279  11.1897  9.18956  6.65126  2.68426   \n",
       "\n",
       "        V76       V77  V78  V79  \n",
       "0  0.346690  0.004209  0.0  0.0  \n",
       "1  0.125949  0.000000  0.0  0.0  \n",
       "\n",
       "[2 rows x 86 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare dataset for final training\n",
    "\n",
    "train_prep_X,train_prep_y,test_cox_X,test_cox_y = prepare_data_set_cox_analysis_and_classification(data_main)\n",
    "train_prep_X.head(2)\n",
    "#X_res_train_temp,Y_res_train_temp,idx = do_sampling_tomelinks(train_prep_X,train_prep_y,'majority')\n",
    "X_res_train_temp,Y_res_train_temp = do_sampling_smote(train_prep_X,train_prep_y,sampling_strategy = 'not minority',smoteenn=True)\n",
    "print('Resampled dataset shape for training set:  %s' % Counter(Y_res_train_temp))\n",
    "\n",
    "print('Preparing Training set after applying SMOTEENN')\n",
    "data_frame_train_X,data_frame_cox_target = create_data_set_for_cox_and_classification_after_smote(X_res_train_temp,Y_res_train_temp,train_prep_X.columns)\n",
    "data_frame_target_train = data_frame_train_X['36months']\n",
    "data_frame_train_set_labels = data_frame_train_X.pop('36months')\n",
    "\n",
    "del data_frame_train_X['months']\n",
    "\n",
    "data_frame_train_X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuLSIF starting...\n",
      "Searching for the optimal sigma and lambda...\n",
      "sigma = 0.10000, lambda = 0.01000, score = 0.00000\n",
      "sigma = 0.10000, lambda = 0.02000, score = 0.00000\n",
      "sigma = 0.10000, lambda = 0.03000, score = 0.00000\n",
      "sigma = 0.10000, lambda = 0.04000, score = 0.00000\n",
      "sigma = 0.10000, lambda = 0.05000, score = 0.00000\n",
      "sigma = 0.30000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.30000, lambda = 0.02000, score = -0.00000\n",
      "sigma = 0.30000, lambda = 0.03000, score = -0.00000\n",
      "sigma = 0.30000, lambda = 0.04000, score = -0.00000\n",
      "sigma = 0.30000, lambda = 0.05000, score = -0.00000\n",
      "sigma = 0.50000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.50000, lambda = 0.02000, score = -0.00000\n",
      "sigma = 0.50000, lambda = 0.03000, score = -0.00000\n",
      "sigma = 0.50000, lambda = 0.04000, score = -0.00000\n",
      "sigma = 0.50000, lambda = 0.05000, score = -0.00000\n",
      "sigma = 0.70000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 0.70000, lambda = 0.02000, score = -0.00000\n",
      "sigma = 0.70000, lambda = 0.03000, score = -0.00000\n",
      "sigma = 0.70000, lambda = 0.04000, score = -0.00000\n",
      "sigma = 0.70000, lambda = 0.05000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.01000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.02000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.03000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.04000, score = -0.00000\n",
      "sigma = 1.00000, lambda = 0.05000, score = -0.00000\n",
      "Found optimal sigma = 1.000, lambda = 0.010.\n",
      "Optimizing theta...\n",
      "Approximate alpha-relative PE-divergence = -0.41\n",
      "Approximate alpha-relative KL-divergence = -inf\n",
      "RuLSIF completed.\n"
     ]
    }
   ],
   "source": [
    "#calculate density estimation ratio in the training set\n",
    "from densratio import densratio\n",
    "np.random.seed(1)\n",
    "alpha = 0\n",
    "densratio_obj = densratio(cov_df_smote_train_data.values, cov_df_smote_test_data.values, alpha=alpha, sigma_range=[0.1, 0.3, 0.5, 0.7, 1], lambda_range=[0.01, 0.02, 0.03, 0.04, 0.05])\n",
    "#densratio_obj = densratio(covariate_train_X.values, covariate_test_X.values, alpha=alpha, sigma_range=[0.1, 0.3, 0.5, 0.7, 1], lambda_range=[0.01, 0.02, 0.03, 0.04, 0.05])\n",
    "density_ratio = densratio_obj.compute_density_ratio(data_frame_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking DONE !\n"
     ]
    }
   ],
   "source": [
    "# extra testing to check the multiplication\n",
    "numpy_data = np.array([[2], [3],[2],[2],[2],[2],[2]])\n",
    "#print(numpy_data)\n",
    "df2 = pd.DataFrame(columns=['dr'],data=numpy_data)\n",
    "# list of strings \n",
    "lst = [1, 2,3,4,5,6,7] \n",
    "  \n",
    "# list of int \n",
    "lst2 = [8, 9, 10, 11, 12, 13, 14] \n",
    "  \n",
    "# Calling DataFrame constructor after zipping \n",
    "# both lists, with columns specified \n",
    "df = pd.DataFrame(list(zip(lst, lst2)), \n",
    "               columns =['Name', 'val']) \n",
    "\n",
    "#print(df2)\n",
    "#print(df)\n",
    "\n",
    "test = df.multiply(df2['dr'], axis=0)\n",
    "#test\n",
    "print(\"Checking DONE !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>antticoagulant_i</th>\n",
       "      <th>hormono</th>\n",
       "      <th>dose_total in the prostate</th>\n",
       "      <th>imrt_only (0=no, 1=yes)</th>\n",
       "      <th>imr_plus_igrt</th>\n",
       "      <th>V0</th>\n",
       "      <th>V1= Volume of the rectum in % receiving a minimum of 1 Gy</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>...</th>\n",
       "      <th>V70</th>\n",
       "      <th>V71</th>\n",
       "      <th>V72</th>\n",
       "      <th>V73</th>\n",
       "      <th>V74</th>\n",
       "      <th>V75</th>\n",
       "      <th>V76</th>\n",
       "      <th>V77</th>\n",
       "      <th>V78</th>\n",
       "      <th>V79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.252104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.296736</td>\n",
       "      <td>23.738872</td>\n",
       "      <td>0.296736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.673591</td>\n",
       "      <td>29.673591</td>\n",
       "      <td>29.673591</td>\n",
       "      <td>29.673591</td>\n",
       "      <td>...</td>\n",
       "      <td>3.933116</td>\n",
       "      <td>3.519199</td>\n",
       "      <td>3.042374</td>\n",
       "      <td>2.535997</td>\n",
       "      <td>1.736795</td>\n",
       "      <td>0.369706</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.397626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.145401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296736</td>\n",
       "      <td>29.673591</td>\n",
       "      <td>29.673591</td>\n",
       "      <td>29.490593</td>\n",
       "      <td>27.710415</td>\n",
       "      <td>...</td>\n",
       "      <td>1.214973</td>\n",
       "      <td>0.880246</td>\n",
       "      <td>0.493401</td>\n",
       "      <td>0.209638</td>\n",
       "      <td>0.045171</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  antticoagulant_i   hormono  dose_total in the prostate  \\\n",
       "0  18.252104               0.0  0.296736                   23.738872   \n",
       "1  18.397626               0.0  0.000000                   23.145401   \n",
       "\n",
       "   imrt_only (0=no, 1=yes)  imr_plus_igrt         V0  \\\n",
       "0                 0.296736       0.000000  29.673591   \n",
       "1                 0.000000       0.296736  29.673591   \n",
       "\n",
       "   V1= Volume of the rectum in % receiving a minimum of 1 Gy         V2  \\\n",
       "0                                          29.673591          29.673591   \n",
       "1                                          29.673591          29.490593   \n",
       "\n",
       "          V3 ...        V70       V71       V72       V73       V74       V75  \\\n",
       "0  29.673591 ...   3.933116  3.519199  3.042374  2.535997  1.736795  0.369706   \n",
       "1  27.710415 ...   1.214973  0.880246  0.493401  0.209638  0.045171  0.001158   \n",
       "\n",
       "        V76  V77  V78  V79  \n",
       "0  0.006627  0.0  0.0  0.0  \n",
       "1  0.000000  0.0  0.0  0.0  \n",
       "\n",
       "[2 rows x 86 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize data according to \n",
    "dr_df = pd.DataFrame(columns=['d_score'],data=density_ratio) \n",
    "\n",
    "data_frame_train_dnn = data_frame_train_X.multiply(dr_df['d_score'],axis=0)\n",
    "data_frame_train_dnn.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>antticoagulant_i</th>\n",
       "      <th>hormono</th>\n",
       "      <th>dose_total in the prostate</th>\n",
       "      <th>imrt_only (0=no, 1=yes)</th>\n",
       "      <th>imr_plus_igrt</th>\n",
       "      <th>V0</th>\n",
       "      <th>V1= Volume of the rectum in % receiving a minimum of 1 Gy</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>...</th>\n",
       "      <th>V70</th>\n",
       "      <th>V71</th>\n",
       "      <th>V72</th>\n",
       "      <th>V73</th>\n",
       "      <th>V74</th>\n",
       "      <th>V75</th>\n",
       "      <th>V76</th>\n",
       "      <th>V77</th>\n",
       "      <th>V78</th>\n",
       "      <th>V79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>61.901370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.10</td>\n",
       "      <td>20.10</td>\n",
       "      <td>18.30</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>6.235556</td>\n",
       "      <td>1.1083</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>56.224658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.998379</td>\n",
       "      <td>99.996759</td>\n",
       "      <td>99.995138</td>\n",
       "      <td>...</td>\n",
       "      <td>4.16</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1.353333</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>0.2080</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age  antticoagulant_i  hormono  dose_total in the prostate  \\\n",
       "408  61.901370               0.0      0.0                        80.0   \n",
       "545  56.224658               0.0      0.0                        78.0   \n",
       "\n",
       "     imrt_only (0=no, 1=yes)  imr_plus_igrt     V0  \\\n",
       "408                      0.0            0.0  100.0   \n",
       "545                      0.0            1.0  100.0   \n",
       "\n",
       "     V1= Volume of the rectum in % receiving a minimum of 1 Gy          V2  \\\n",
       "408                                         100.000000          100.000000   \n",
       "545                                          99.998379           99.996759   \n",
       "\n",
       "             V3 ...     V70    V71    V72        V73       V74     V75  \\\n",
       "408  100.000000 ...   20.10  20.10  18.30  18.100000  6.235556  1.1083   \n",
       "545   99.995138 ...    4.16   3.37   2.44   1.353333  0.555000  0.2080   \n",
       "\n",
       "          V76    V77  V78  V79  \n",
       "408  0.636000  0.000  0.0  0.0  \n",
       "545  0.061538  0.012  0.0  0.0  \n",
       "\n",
       "[2 rows x 86 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Debugging Code\n",
    "test_X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>antticoagulant_i</th>\n",
       "      <th>hormono</th>\n",
       "      <th>dose_total in the prostate</th>\n",
       "      <th>imrt_only (0=no, 1=yes)</th>\n",
       "      <th>imr_plus_igrt</th>\n",
       "      <th>V0</th>\n",
       "      <th>V1= Volume of the rectum in % receiving a minimum of 1 Gy</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>...</th>\n",
       "      <th>V70</th>\n",
       "      <th>V71</th>\n",
       "      <th>V72</th>\n",
       "      <th>V73</th>\n",
       "      <th>V74</th>\n",
       "      <th>V75</th>\n",
       "      <th>V76</th>\n",
       "      <th>V77</th>\n",
       "      <th>V78</th>\n",
       "      <th>V79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [age, antticoagulant_i, hormono, dose_total in the prostate, imrt_only (0=no, 1=yes), imr_plus_igrt, V0, V1= Volume of the rectum in % receiving a minimum of 1 Gy, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, V29, V30, V31, V32, V33, V34, V35, V36, V37, V38, V39, V40, V41, V42, V43, V44, V45, V46, V47, V48, V49, V50, V51, V52, V53, V54, V55, V56, V57, V58, V59, V60, V61, V62, V63, V64, V65, V66, V67, V68, V69, V70, V71, V72, V73, V74, V75, V76, V77, V78, V79]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 86 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Debugging Code\n",
    "del data_frame_train_X['36months']\n",
    "data_frame_train_X.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 86)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Debugging Code , don't uncomment unless you check something\n",
    "data_frame_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging Code\n",
    "sample_weights = np.ones(shape=(len(density_ratio),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Debugging code\n",
    "sample_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 607 samples, validate on 254 samples\n",
      "Epoch 1/15\n",
      "607/607 [==============================] - 0s 814us/sample - loss: 0.1934 - accuracy: 0.4646 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 2/15\n",
      "607/607 [==============================] - 0s 71us/sample - loss: 0.1923 - accuracy: 0.4926 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 3/15\n",
      "607/607 [==============================] - 0s 65us/sample - loss: 0.2029 - accuracy: 0.4909 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 4/15\n",
      "607/607 [==============================] - 0s 65us/sample - loss: 0.1818 - accuracy: 0.4992 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 5/15\n",
      "607/607 [==============================] - 0s 264us/sample - loss: 0.1860 - accuracy: 0.4992 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 6/15\n",
      "607/607 [==============================] - 0s 170us/sample - loss: 0.1770 - accuracy: 0.4778 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 7/15\n",
      "607/607 [==============================] - 0s 113us/sample - loss: 0.1682 - accuracy: 0.4778 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 8/15\n",
      "607/607 [==============================] - 0s 87us/sample - loss: 0.1686 - accuracy: 0.4860 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 9/15\n",
      "607/607 [==============================] - 0s 78us/sample - loss: 0.1708 - accuracy: 0.4926 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 10/15\n",
      "607/607 [==============================] - 0s 62us/sample - loss: 0.1591 - accuracy: 0.4860 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 11/15\n",
      "607/607 [==============================] - 0s 71us/sample - loss: 0.1759 - accuracy: 0.4893 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 12/15\n",
      "607/607 [==============================] - 0s 64us/sample - loss: 0.1498 - accuracy: 0.4893 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 13/15\n",
      "607/607 [==============================] - 0s 65us/sample - loss: 0.1703 - accuracy: 0.4992 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 14/15\n",
      "607/607 [==============================] - 0s 73us/sample - loss: 0.1518 - accuracy: 0.4926 - val_loss: 1.3961 - val_accuracy: 0.9134\n",
      "Epoch 15/15\n",
      "607/607 [==============================] - 0s 75us/sample - loss: 0.1511 - accuracy: 0.4975 - val_loss: 1.3961 - val_accuracy: 0.9134\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "encoded_Y = convert_to_encoding(data_frame_target_train)\n",
    "# baseline model - results using model_1\n",
    "\n",
    "def create_model_done():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200,input_shape=(86,), activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    \n",
    "    model.add(Dense(100, activation=\"relu\",kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, activation=\"relu\",kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, activation=\"relu\",kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(25, activation=\"relu\",kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200,input_shape=(86,), activation='relu'))\n",
    "    \n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(25, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights  = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(data_frame_target_train),\n",
    "                                                 data_frame_target_train)   \n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "model = create_model()\n",
    "#history = model.fit(data_frame_train_X.values,encoded_Y,epochs=15, batch_size=32,verbose=1,sample_weight=density_ratio,class_weight=class_weights)\n",
    "#history = model.fit(data_frame_train_X.values,encoded_Y,epochs=15, batch_size=32,verbose=1,sample_weight=density_ratio,validation_data=(test_X,target_test))\n",
    "history = model.fit(data_frame_train_X.values,encoded_Y,epochs=15, batch_size=32,verbose=1,sample_weight=density_ratio)\n",
    "\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "#estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "#kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "#results = cross_val_score(estimator, data_frame_train_X.values, encoded_Y, cv=kfold)\n",
    "#print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607/607 [==============================] - 0s 163us/sample - loss: 8.1785 - accuracy: 0.4926\n",
      "Loss: 8.179\n",
      "Accuracy: 0.493\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = model.evaluate(data_frame_train_X, encoded_Y, batch_size=32)\n",
    "print(\"Loss: \"+ str(np.round(train_loss, 3)))\n",
    "print(\"Accuracy: \"+ str(np.round(train_accuracy, 3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy for RF:  50.0\n",
      "MCC for RF:  0.0\n",
      "ROC :  50.0\n",
      "Confusion Matrix: \n",
      "[[232   0]\n",
      " [ 22   0]]\n",
      "Sensitivity: 0.00: and Specificity:1.00\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "cm,predictions,test_Y = predict_and_create_cm(model,test_X,target_test)\n",
    "\n",
    "balanced_accuracy = recall_score(test_Y,predictions, average='macro') \n",
    "mcc_coef= matthews_corrcoef(test_Y, predictions.round())\n",
    "roc_score = roc_auc_score (test_Y,predictions.round())\n",
    "\n",
    "print('Balanced Accuracy for RF: ' , round(balanced_accuracy,2)*100)\n",
    "print('MCC for RF: ' , mcc_coef)\n",
    "print('ROC : ' , round(roc_score,2)*100)\n",
    "print('Confusion Matrix: ')\n",
    "print(cm)\n",
    "sensititvity,specificity = calculate_custom_sensitivity(cm)\n",
    "print(\"Sensitivity: {:0.2f}: and Specificity:{:0.2f}\".format(sensititvity,specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing duplicates:  (861, 87)\n",
      "Resampled dataset shape for training set:  Counter({1: 384, 0: 265})\n",
      "Apply pseudo labeling\n",
      "Train on 649 samples\n",
      "Epoch 1/15\n",
      "649/649 [==============================] - 0s 533us/sample - loss: 6.7782 - accuracy: 0.4807\n",
      "Epoch 2/15\n",
      "649/649 [==============================] - 0s 56us/sample - loss: 3.8016 - accuracy: 0.5316\n",
      "Epoch 3/15\n",
      "649/649 [==============================] - 0s 83us/sample - loss: 1.8531 - accuracy: 0.5085\n",
      "Epoch 4/15\n",
      "649/649 [==============================] - 0s 56us/sample - loss: 1.0098 - accuracy: 0.5208\n",
      "Epoch 5/15\n",
      "649/649 [==============================] - 0s 209us/sample - loss: 0.8620 - accuracy: 0.5501\n",
      "Epoch 6/15\n",
      "649/649 [==============================] - 0s 108us/sample - loss: 0.7420 - accuracy: 0.5593\n",
      "Epoch 7/15\n",
      "649/649 [==============================] - 0s 77us/sample - loss: 0.7377 - accuracy: 0.5609\n",
      "Epoch 8/15\n",
      "649/649 [==============================] - 0s 57us/sample - loss: 0.7756 - accuracy: 0.5686\n",
      "Epoch 9/15\n",
      "649/649 [==============================] - 0s 71us/sample - loss: 0.7381 - accuracy: 0.5439\n",
      "Epoch 10/15\n",
      "649/649 [==============================] - 0s 62us/sample - loss: 0.7303 - accuracy: 0.5485\n",
      "Epoch 11/15\n",
      "649/649 [==============================] - 0s 74us/sample - loss: 0.7136 - accuracy: 0.5609\n",
      "Epoch 12/15\n",
      "649/649 [==============================] - 0s 56us/sample - loss: 0.7139 - accuracy: 0.5794\n",
      "Epoch 13/15\n",
      "649/649 [==============================] - 0s 66us/sample - loss: 0.6854 - accuracy: 0.6071\n",
      "Epoch 14/15\n",
      "649/649 [==============================] - 0s 64us/sample - loss: 0.6874 - accuracy: 0.5917\n",
      "Epoch 15/15\n",
      "649/649 [==============================] - 0s 55us/sample - loss: 0.6782 - accuracy: 0.6287\n",
      "Balanced Accuracy for RF:  52.0\n",
      "MCC for RF:  0.025744128061607568\n",
      "ROC :  52.0\n",
      "Confusion Matrix: \n",
      "[[ 84 148]\n",
      " [  7  15]]\n",
      "Sensitivity: 0.68: and Specificity:0.36\n"
     ]
    }
   ],
   "source": [
    "#apply pseudo labeling and test it again\n",
    "\n",
    "merged_df_rf= create_merged_data_frame(model,test_X,data_frame_train_X,data_frame_target_train,data_frame_train_X.columns)\n",
    "merged_df_rf = shuffle(merged_df_rf)\n",
    "merged_df_rf.drop_duplicates(keep='first', inplace=True)\n",
    "print('Shape after removing duplicates: ', merged_df_rf.shape) \n",
    "\n",
    "\n",
    "augmented_x,augmented_test_y = pop_labels(merged_df_rf,'36months')\n",
    "X_res_train_augmented,Y_res_train_augmented = do_sampling_smote(augmented_x,augmented_test_y,sampling_strategy = 'not minority',smoteenn=True)\n",
    "#print(sorted(Counter(y_res).items()))\n",
    "print('Resampled dataset shape for training set:  %s' % Counter(Y_res_train_augmented))\n",
    "\n",
    "print('Apply pseudo labeling')\n",
    "\n",
    "#rf_clf.fit(augmented_x,augmented_test_y)\n",
    "#model.fit(X_res_train_augmented,Y_res_train_augmented)\n",
    "\n",
    "class_weights  = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(Y_res_train_augmented),\n",
    "                                                 Y_res_train_augmented)   \n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "model = create_model()\n",
    "#model.load_weights(initial_weights)\n",
    "\n",
    "\n",
    "history = model.fit(X_res_train_augmented,Y_res_train_augmented,epochs=15, batch_size=32,verbose=1)\n",
    "#history = model.fit(X_res_train_augmented,Y_res_train_augmented,epochs=15, batch_size=32,verbose=1,class_weight=class_weights)\n",
    "\n",
    "\n",
    "#history = model.fit(augmented_x,augmented_test_y,epochs=15, batch_size=32,verbose=1)\n",
    "#history = model.fit(augmented_x,augmented_test_y,epochs=100, batch_size=32,verbose=1,class_weight=class_weights)\n",
    "\n",
    "cm,predictions,test_Y = predict_and_create_cm(model,test_X,target_test)\n",
    "\n",
    "balanced_accuracy = recall_score(test_Y,predictions, average='macro') \n",
    "mcc_coef= matthews_corrcoef(test_Y, predictions)\n",
    "roc_score = roc_auc_score (test_Y,predictions)\n",
    "\n",
    "print('Balanced Accuracy for RF: ' , round(balanced_accuracy,2)*100)\n",
    "print('MCC for RF: ' , mcc_coef)\n",
    "print('ROC : ' , round(roc_score,2)*100)\n",
    "print('Confusion Matrix: ')\n",
    "print(cm)\n",
    "sensititvity,specificity = calculate_custom_sensitivity(cm)\n",
    "print(\"Sensitivity: {:0.2f}: and Specificity:{:0.2f}\".format(sensititvity,specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 337 samples\n",
      "Epoch 1/15\n",
      "337/337 [==============================] - 0s 1ms/sample - loss: 2.0690 - accuracy: 0.8368\n",
      "Epoch 2/15\n",
      "337/337 [==============================] - 0s 81us/sample - loss: 1.2913 - accuracy: 0.9169\n",
      "Epoch 3/15\n",
      "337/337 [==============================] - 0s 105us/sample - loss: 1.2915 - accuracy: 0.9199\n",
      "Epoch 4/15\n",
      "337/337 [==============================] - 0s 71us/sample - loss: 1.3182 - accuracy: 0.9139\n",
      "Epoch 5/15\n",
      "337/337 [==============================] - 0s 88us/sample - loss: 1.3560 - accuracy: 0.9139\n",
      "Epoch 6/15\n",
      "337/337 [==============================] - 0s 81us/sample - loss: 1.2926 - accuracy: 0.9199\n",
      "Epoch 7/15\n",
      "337/337 [==============================] - 0s 66us/sample - loss: 1.2914 - accuracy: 0.9199\n",
      "Epoch 8/15\n",
      "337/337 [==============================] - 0s 71us/sample - loss: 1.2914 - accuracy: 0.9199\n",
      "Epoch 9/15\n",
      "337/337 [==============================] - 0s 79us/sample - loss: 1.2914 - accuracy: 0.9199\n",
      "Epoch 10/15\n",
      "337/337 [==============================] - 0s 75us/sample - loss: 1.2914 - accuracy: 0.9199\n",
      "Epoch 11/15\n",
      "337/337 [==============================] - 0s 64us/sample - loss: 1.2914 - accuracy: 0.9199\n",
      "Epoch 12/15\n",
      "337/337 [==============================] - 0s 64us/sample - loss: 1.2914 - accuracy: 0.9199\n",
      "Epoch 13/15\n",
      "337/337 [==============================] - 0s 73us/sample - loss: 1.2914 - accuracy: 0.9199\n",
      "Epoch 14/15\n",
      "337/337 [==============================] - 0s 66us/sample - loss: 1.2914 - accuracy: 0.9199\n",
      "Epoch 15/15\n",
      "337/337 [==============================] - 0s 65us/sample - loss: 1.2914 - accuracy: 0.9199\n"
     ]
    }
   ],
   "source": [
    "#VANILA DNN without using the framework\n",
    "\n",
    "encoded_Y = convert_to_encoding(target_train)\n",
    "\n",
    "model = create_model()\n",
    "#history = model.fit(data_frame_train_X.values,encoded_Y,epochs=15, batch_size=32,verbose=1,sample_weight=density_ratio,class_weight=class_weights)\n",
    "history = model.fit(train_X.values,encoded_Y,epochs=15, batch_size=32,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 0s 259us/sample - loss: 1.2914 - accuracy: 0.9199\n",
      "Loss: 1.291\n",
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = model.evaluate(train_X, encoded_Y, batch_size=32)\n",
    "print(\"Loss: \"+ str(np.round(train_loss, 3)))\n",
    "print(\"Accuracy: \"+ str(np.round(train_accuracy, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy for RF:  50.0\n",
      "MCC for RF:  0.0\n",
      "ROC :  50.0\n",
      "Confusion Matrix: \n",
      "[[232   0]\n",
      " [ 22   0]]\n",
      "Sensitivity: 0.00: and Specificity:1.00\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "cm,predictions,test_Y = predict_and_create_cm(model,test_X,target_test)\n",
    "\n",
    "balanced_accuracy = recall_score(test_Y,predictions, average='macro') \n",
    "mcc_coef= matthews_corrcoef(test_Y, predictions.round())\n",
    "roc_score = roc_auc_score (test_Y,predictions.round())\n",
    "\n",
    "print('Balanced Accuracy for RF: ' , round(balanced_accuracy,2)*100)\n",
    "print('MCC for RF: ' , mcc_coef)\n",
    "print('ROC : ' , round(roc_score,2)*100)\n",
    "print('Confusion Matrix: ')\n",
    "print(cm)\n",
    "sensititvity,specificity = calculate_custom_sensitivity(cm)\n",
    "print(\"Sensitivity: {:0.2f}: and Specificity:{:0.2f}\".format(sensititvity,specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dose  Feat  Class\n",
      "0     1     5      0\n",
      "1     2     6      0\n",
      "2     3     7      1\n",
      "3     4     8      1\n"
     ]
    }
   ],
   "source": [
    "#DEBUGGING PURPOSE CODE,\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cars = {'Dose': [1,2,3,4],\n",
    "        'Feat': [5,6,7,8],\n",
    "        'Class': [0,0,1,1]\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(cars, columns = ['Dose', 'Feat','Class'])\n",
    "\n",
    "print (df) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
